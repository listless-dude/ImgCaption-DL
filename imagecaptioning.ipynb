{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Understanding the dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport keras\nimport re\nimport string\nimport cv2\nimport json\nimport pickle\nimport collections\nfrom keras.applications.resnet import ResNet50, preprocess_input\nfrom keras.preprocessing import image\nfrom keras.models import Model, load_model\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.layers import Input, Dense, Dropout, Embedding, LSTM\nfrom keras.layers.merge import add\n","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:46:02.214416Z","iopub.execute_input":"2022-09-05T16:46:02.215117Z","iopub.status.idle":"2022-09-05T16:46:02.222623Z","shell.execute_reply.started":"2022-09-05T16:46:02.215079Z","shell.execute_reply":"2022-09-05T16:46:02.221023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def readTxtFile(path):\n    with open(path) as f:\n        captions = f.read()\n    return captions","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:46:02.224786Z","iopub.execute_input":"2022-09-05T16:46:02.225160Z","iopub.status.idle":"2022-09-05T16:46:02.237025Z","shell.execute_reply.started":"2022-09-05T16:46:02.225123Z","shell.execute_reply":"2022-09-05T16:46:02.235734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# read from the input that contains captions of each image\ncaptions = readTxtFile(\"../input/flickr8k/captions.txt\")","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:46:02.239679Z","iopub.execute_input":"2022-09-05T16:46:02.240231Z","iopub.status.idle":"2022-09-05T16:46:02.272441Z","shell.execute_reply.started":"2022-09-05T16:46:02.240197Z","shell.execute_reply":"2022-09-05T16:46:02.271208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"captions = captions.split('\\n')[1:-1] #rejecting the first and last row as it is redundant","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:46:02.277604Z","iopub.execute_input":"2022-09-05T16:46:02.278116Z","iopub.status.idle":"2022-09-05T16:46:02.295062Z","shell.execute_reply.started":"2022-09-05T16:46:02.278063Z","shell.execute_reply":"2022-09-05T16:46:02.293824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(captions))\nprint(captions[0])\nprint(captions[1])\nprint(captions[2])\ncaptions[0].split(\".jpg,\")","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:46:02.299073Z","iopub.execute_input":"2022-09-05T16:46:02.300080Z","iopub.status.idle":"2022-09-05T16:46:02.329484Z","shell.execute_reply.started":"2022-09-05T16:46:02.300039Z","shell.execute_reply":"2022-09-05T16:46:02.327909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dictionary to map image ids with their respective captions\ndescriptions = {}\n\nfor x in captions:\n    img_name, img_desc = x.split(\".jpg,\")\n    # if the img_name is not present\n    if descriptions.get(img_name) is None:\n        descriptions[img_name] = []\n    descriptions[img_name].append(img_desc)","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:46:02.331109Z","iopub.execute_input":"2022-09-05T16:46:02.335850Z","iopub.status.idle":"2022-09-05T16:46:02.433051Z","shell.execute_reply.started":"2022-09-05T16:46:02.335814Z","shell.execute_reply":"2022-09-05T16:46:02.429716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking if we got our dictionary correctly\n# we have 5 captions for each image in the dataset\ndescriptions[\"1000268201_693b08cb0e\"]","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:46:02.438656Z","iopub.execute_input":"2022-09-05T16:46:02.443041Z","iopub.status.idle":"2022-09-05T16:46:02.456696Z","shell.execute_reply.started":"2022-09-05T16:46:02.442994Z","shell.execute_reply":"2022-09-05T16:46:02.455273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMG_PATH = \"../input/flickr8k/Images/\"\n\ndef readImg(imgId):\n    img = cv2.imread(IMG_PATH + imgId + \".jpg\")# checking our image with a id\n    # since cv2 reads our image in bgr format, we convert it to RGB\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.imshow(img)\n    plt.axis('off')\n    #plt.title(descriptions[imgId][0])\n    plt.show()\nreadImg(\"1001773457_577c3a7d70\")\ndescriptions[\"1001773457_577c3a7d70\"]","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:46:02.458305Z","iopub.execute_input":"2022-09-05T16:46:02.459997Z","iopub.status.idle":"2022-09-05T16:46:02.714978Z","shell.execute_reply.started":"2022-09-05T16:46:02.459952Z","shell.execute_reply":"2022-09-05T16:46:02.713753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Key points from the data set:**\n* Has /Images folder with 8000 images\n* Has a captions.txt file which has 5 captions for each image\n* The captions are mapped with the image id","metadata":{}},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"code","source":"def clean(sentence):\n    sentence = sentence.lower()\n    sentence = re.sub(\"[^a-z]+\", \" \", sentence) # substitute that is not an alphabet replace with space\n    sentence = sentence.split()\n    sentence = [s for s in sentence if len(s) > 1] # reject all words of length 1\n    sentence = \" \".join(sentence)\n    return sentence","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:46:02.716533Z","iopub.execute_input":"2022-09-05T16:46:02.717022Z","iopub.status.idle":"2022-09-05T16:46:02.724220Z","shell.execute_reply.started":"2022-09-05T16:46:02.716984Z","shell.execute_reply":"2022-09-05T16:46:02.722914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# iterate over all (key, value) pairs\nfor key,caption in descriptions.items():\n    # iterate over all captions i.e. 5\n    for i in range(len(caption)):\n        # clean ith caption\n        caption[i] = clean(caption[i])","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:46:02.729192Z","iopub.execute_input":"2022-09-05T16:46:02.733356Z","iopub.status.idle":"2022-09-05T16:46:03.106244Z","shell.execute_reply.started":"2022-09-05T16:46:02.733320Z","shell.execute_reply":"2022-09-05T16:46:03.105242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"descriptions[\"1001773457_577c3a7d70\"]","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:46:03.107888Z","iopub.execute_input":"2022-09-05T16:46:03.108253Z","iopub.status.idle":"2022-09-05T16:46:03.116340Z","shell.execute_reply.started":"2022-09-05T16:46:03.108216Z","shell.execute_reply":"2022-09-05T16:46:03.115288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save our newly made dictionary after cleaning\n# so that we can save time for large datasets\nwith open(\"descriptions.txt\", \"w\") as f:\n    f.write(str(descriptions))","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:46:03.118147Z","iopub.execute_input":"2022-09-05T16:46:03.118876Z","iopub.status.idle":"2022-09-05T16:46:03.149675Z","shell.execute_reply.started":"2022-09-05T16:46:03.118840Z","shell.execute_reply":"2022-09-05T16:46:03.148557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Vocabulary\nSet of unique words model can predict, i.e. trace back our probability output number to map from the vocab","metadata":{}},{"cell_type":"code","source":"# using json.load() to read the dictionary into python\ndescriptions = None\nwith open(\"descriptions.txt\") as f:\n    descriptions = f.read()\njson_str = descriptions.replace(\"'\", \"\\\"\")\ndescriptions = json.loads(json_str)","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:46:03.152177Z","iopub.execute_input":"2022-09-05T16:46:03.152961Z","iopub.status.idle":"2022-09-05T16:46:03.177931Z","shell.execute_reply.started":"2022-09-05T16:46:03.152923Z","shell.execute_reply":"2022-09-05T16:46:03.177053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(type(descriptions))","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:46:03.179403Z","iopub.execute_input":"2022-09-05T16:46:03.179795Z","iopub.status.idle":"2022-09-05T16:46:03.185918Z","shell.execute_reply.started":"2022-09-05T16:46:03.179759Z","shell.execute_reply":"2022-09-05T16:46:03.184801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab = set()\nfor key in descriptions.keys():\n    # for each sentence we split it into words and pass it to set vocab from descriptions dictionary\n    [vocab.update(sentence.split()) for sentence in descriptions[key]]\nprint(\"Vocab Size : %d\"% len(vocab))","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:46:03.187745Z","iopub.execute_input":"2022-09-05T16:46:03.188441Z","iopub.status.idle":"2022-09-05T16:46:03.249844Z","shell.execute_reply.started":"2022-09-05T16:46:03.188405Z","shell.execute_reply":"2022-09-05T16:46:03.248678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_words = []\nfor key in descriptions.keys():\n    [total_words.append(i) for des in descriptions[key] for i in des.split()]\nprint(\"Total Word %d\"%len(total_words))","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:46:03.251303Z","iopub.execute_input":"2022-09-05T16:46:03.252426Z","iopub.status.idle":"2022-09-05T16:46:03.334458Z","shell.execute_reply.started":"2022-09-05T16:46:03.252387Z","shell.execute_reply":"2022-09-05T16:46:03.333348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"counter = collections.Counter(total_words)\nfreq_count = dict(counter)","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:46:03.336411Z","iopub.execute_input":"2022-09-05T16:46:03.336821Z","iopub.status.idle":"2022-09-05T16:46:03.384723Z","shell.execute_reply.started":"2022-09-05T16:46:03.336784Z","shell.execute_reply":"2022-09-05T16:46:03.383598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sort our frequency count based on value\nsorted_freq_count = sorted(freq_count.items(), reverse=True, key=lambda x : x[1])\nthreshold_freq = 10\n# we reject all the words whose frequency is less than 10\nsorted_freq_count = [x for x in sorted_freq_count if x[1] > threshold_freq]\ntotal_words = [x[0] for x in sorted_freq_count]","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:46:03.386458Z","iopub.execute_input":"2022-09-05T16:46:03.386902Z","iopub.status.idle":"2022-09-05T16:46:03.415687Z","shell.execute_reply.started":"2022-09-05T16:46:03.386866Z","shell.execute_reply":"2022-09-05T16:46:03.414512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(total_words)) # final vocab size","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:46:03.417419Z","iopub.execute_input":"2022-09-05T16:46:03.417848Z","iopub.status.idle":"2022-09-05T16:46:03.426398Z","shell.execute_reply.started":"2022-09-05T16:46:03.417811Z","shell.execute_reply":"2022-09-05T16:46:03.425409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing Training Data","metadata":{}},{"cell_type":"code","source":"data = []\nfor k in descriptions.keys():\n    data.append(k)\nprint(len(data))","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:46:03.427779Z","iopub.execute_input":"2022-09-05T16:46:03.428727Z","iopub.status.idle":"2022-09-05T16:46:03.439251Z","shell.execute_reply.started":"2022-09-05T16:46:03.428691Z","shell.execute_reply":"2022-09-05T16:46:03.438140Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"splitSize = int(0.8 * len(data)) # 80-20 partition for training and test data\ntrain_data = data[ : splitSize]\ntest_data = data[splitSize : ]\nprint(len(train_data), len(test_data))","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:46:03.441145Z","iopub.execute_input":"2022-09-05T16:46:03.441576Z","iopub.status.idle":"2022-09-05T16:46:03.451215Z","shell.execute_reply.started":"2022-09-05T16:46:03.441519Z","shell.execute_reply":"2022-09-05T16:46:03.450093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add <start> and <end> token to our training data\n# this determines where to start and where to stop\ntrain_descriptions = {}\nfor img_id in train_data:\n    train_descriptions[img_id] = []\n    for cap in descriptions[img_id]:\n        cap_ = \"startseq \" + cap + \" endseq\"\n        train_descriptions[img_id].append(cap_)","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:46:03.453306Z","iopub.execute_input":"2022-09-05T16:46:03.453861Z","iopub.status.idle":"2022-09-05T16:46:03.482544Z","shell.execute_reply.started":"2022-09-05T16:46:03.453825Z","shell.execute_reply":"2022-09-05T16:46:03.481585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_descriptions[\"1001773457_577c3a7d70\"]\n# each sentence has 'startseq' and 'endseq'","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:46:03.483596Z","iopub.execute_input":"2022-09-05T16:46:03.483849Z","iopub.status.idle":"2022-09-05T16:46:03.491036Z","shell.execute_reply.started":"2022-09-05T16:46:03.483826Z","shell.execute_reply":"2022-09-05T16:46:03.489832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transfer Learning \nTo extract features from images and text","metadata":{}},{"cell_type":"markdown","source":"## Features from Images","metadata":{}},{"cell_type":"code","source":"# using pretrained model RESNET50\nmodel = ResNet50(weights=\"imagenet\", input_shape=(224,224,3))\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:46:03.492725Z","iopub.execute_input":"2022-09-05T16:46:03.493335Z","iopub.status.idle":"2022-09-05T16:46:04.918412Z","shell.execute_reply.started":"2022-09-05T16:46:03.493297Z","shell.execute_reply":"2022-09-05T16:46:04.917411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.layers[-2]","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:46:04.924862Z","iopub.execute_input":"2022-09-05T16:46:04.925154Z","iopub.status.idle":"2022-09-05T16:46:04.931850Z","shell.execute_reply.started":"2022-09-05T16:46:04.925127Z","shell.execute_reply":"2022-09-05T16:46:04.930748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we want to create a use the CONV model but upto the global average pooling layer\nmodel_new = Model(model.input, model.layers[-2].output)","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:46:04.933664Z","iopub.execute_input":"2022-09-05T16:46:04.934479Z","iopub.status.idle":"2022-09-05T16:46:04.955354Z","shell.execute_reply.started":"2022-09-05T16:46:04.934435Z","shell.execute_reply":"2022-09-05T16:46:04.954470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_img(img):\n    img = image.load_img(img, target_size=(224,224))\n    img = image.img_to_array(img)\n    img = np.expand_dims(img, axis=0) # [Batchsize, 224, 224, 3] extend 3d tensor to 4d\n    # Normalisation\n    img = preprocess_input(img)\n    return img","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:46:04.957067Z","iopub.execute_input":"2022-09-05T16:46:04.957326Z","iopub.status.idle":"2022-09-05T16:46:04.964986Z","shell.execute_reply.started":"2022-09-05T16:46:04.957301Z","shell.execute_reply":"2022-09-05T16:46:04.963654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = preprocess_img(IMG_PATH+\"1001773457_577c3a7d70.jpg\")\nplt.imshow(img[0])\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:46:04.967204Z","iopub.execute_input":"2022-09-05T16:46:04.968043Z","iopub.status.idle":"2022-09-05T16:46:05.128421Z","shell.execute_reply.started":"2022-09-05T16:46:04.968007Z","shell.execute_reply":"2022-09-05T16:46:05.127146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode_img(img):\n    img = preprocess_img(img) # preprocess the image and normalize it\n    feature_vector = model_new.predict(img) # passing through resnet conv layers\n    # print(feature_vector.shape) # (1, 2048)\n    feature_vector = feature_vector.reshape((-1,))\n    return feature_vector","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:46:05.130147Z","iopub.execute_input":"2022-09-05T16:46:05.132775Z","iopub.status.idle":"2022-09-05T16:46:05.138701Z","shell.execute_reply.started":"2022-09-05T16:46:05.132736Z","shell.execute_reply":"2022-09-05T16:46:05.137679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encode_img(IMG_PATH+\"1001773457_577c3a7d70.jpg\")","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:46:05.140049Z","iopub.execute_input":"2022-09-05T16:46:05.141080Z","iopub.status.idle":"2022-09-05T16:46:05.960870Z","shell.execute_reply.started":"2022-09-05T16:46:05.141042Z","shell.execute_reply":"2022-09-05T16:46:05.959741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from time import time\nencoding_train = {}\n# img_id -> feature_vector\nstart_t = time()\nfor i, img_id in enumerate(train_data):\n    img_path = IMG_PATH+img_id+\".jpg\"\n    encoding_train[img_id] = encode_img(img_path)\n    \n    if i%100==0:\n        print(\"Encoding Progress %d\"%i)\nend_t = time()\nprint(\"Total Time Taken :\", end_t-start_t)","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:46:05.963635Z","iopub.execute_input":"2022-09-05T16:46:05.964336Z","iopub.status.idle":"2022-09-05T16:52:44.162313Z","shell.execute_reply.started":"2022-09-05T16:46:05.964295Z","shell.execute_reply":"2022-09-05T16:52:44.161221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# store the trained data from the resnet locally\nencoding_train[\"1001773457_577c3a7d70\"]","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:52:44.163837Z","iopub.execute_input":"2022-09-05T16:52:44.164173Z","iopub.status.idle":"2022-09-05T16:52:44.173915Z","shell.execute_reply.started":"2022-09-05T16:52:44.164138Z","shell.execute_reply":"2022-09-05T16:52:44.172585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"encoded_train_features.pkl\",\"wb\") as f:\n    pickle.dump(encoding_train, f)","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:52:44.175652Z","iopub.execute_input":"2022-09-05T16:52:44.176321Z","iopub.status.idle":"2022-09-05T16:52:44.340405Z","shell.execute_reply.started":"2022-09-05T16:52:44.176283Z","shell.execute_reply":"2022-09-05T16:52:44.339422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoding_test = {}\n# img_id -> feature_vector\nstart_t = time()\nfor i, img_id in enumerate(test_data):\n    img_path = IMG_PATH+img_id+\".jpg\"\n    encoding_test[img_id] = encode_img(img_path)\n    \n    if i%100==0:\n        print(\"Encoding Progress %d\"%i)\nend_t = time()\nprint(\"Total Time Taken :\", end_t-start_t)","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:52:44.342075Z","iopub.execute_input":"2022-09-05T16:52:44.342425Z","iopub.status.idle":"2022-09-05T16:54:23.890117Z","shell.execute_reply.started":"2022-09-05T16:52:44.342386Z","shell.execute_reply":"2022-09-05T16:54:23.889146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"encoded_test_features.pkl\", \"wb\") as f:\n    pickle.dump(encoding_test, f)","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:54:23.891401Z","iopub.execute_input":"2022-09-05T16:54:23.891774Z","iopub.status.idle":"2022-09-05T16:54:23.934496Z","shell.execute_reply.started":"2022-09-05T16:54:23.891737Z","shell.execute_reply":"2022-09-05T16:54:23.933522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing Captions","metadata":{}},{"cell_type":"code","source":"# vocab\nlen(total_words)","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:54:23.936035Z","iopub.execute_input":"2022-09-05T16:54:23.936619Z","iopub.status.idle":"2022-09-05T16:54:23.944094Z","shell.execute_reply.started":"2022-09-05T16:54:23.936577Z","shell.execute_reply":"2022-09-05T16:54:23.943065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_to_index = {}\nindex_to_word = {}\nfor i, word in enumerate(total_words):\n    word_to_index[word] = i+1;\n    index_to_word[i+1] = word ","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:54:23.945766Z","iopub.execute_input":"2022-09-05T16:54:23.946775Z","iopub.status.idle":"2022-09-05T16:54:23.955157Z","shell.execute_reply.started":"2022-09-05T16:54:23.946734Z","shell.execute_reply":"2022-09-05T16:54:23.954079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(word_to_index[\"on\"], index_to_word[7])","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:54:23.957052Z","iopub.execute_input":"2022-09-05T16:54:23.957514Z","iopub.status.idle":"2022-09-05T16:54:23.965347Z","shell.execute_reply.started":"2022-09-05T16:54:23.957465Z","shell.execute_reply":"2022-09-05T16:54:23.964171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index_to_word[1846] = \"startseq\"\nword_to_index[\"startseq\"] = 1846\n\nindex_to_word[1847] = \"endseq\"\nword_to_index[\"endseq\"] = 1847\n\nvocab_size = len(word_to_index) + 1\nprint(vocab_size)","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:54:23.967078Z","iopub.execute_input":"2022-09-05T16:54:23.967719Z","iopub.status.idle":"2022-09-05T16:54:23.975617Z","shell.execute_reply.started":"2022-09-05T16:54:23.967671Z","shell.execute_reply":"2022-09-05T16:54:23.974042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pickle\nmax_len = 0\nfor key in train_descriptions.keys():\n    for cap in train_descriptions[key]:\n        max_len = max(max_len, len(cap.split()))\nprint(max_len)","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:54:23.977162Z","iopub.execute_input":"2022-09-05T16:54:23.979858Z","iopub.status.idle":"2022-09-05T16:54:24.025685Z","shell.execute_reply.started":"2022-09-05T16:54:23.979753Z","shell.execute_reply":"2022-09-05T16:54:24.024846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Supervised Learning Problem\nLanguage Modelling: \nProbabilities of getting a output depends on all the previous outputs\nP(W_t+1 | W1.....Wt)\n","metadata":{}},{"cell_type":"code","source":"# data generator\ndef data_generator(train_descriptions, encoding_train, word_to_index, max_len, batch_size):\n    X1, X2, y = [], [], [] #image vector X1, partial vector X2, target word y\n    n = 0\n    while True:\n        for key, desc_list in train_descriptions.items():\n            n += 1\n            photo = encoding_train[key]\n            for capt in desc_list:\n                seq = [word_to_index[word] for word in capt.split() if word in word_to_index]\n                for i in range(1, len(seq)):\n                    xi = seq[0:i]\n                    yi = seq[i]\n                    \n                    # zero padding\n                    # returns a 2d matrix\n                    xi = pad_sequences([xi], maxlen=max_len, value=0, padding='post')[0]\n                    yi = to_categorical([yi], num_classes=vocab_size)[0]\n                    \n                    X1.append(photo)\n                    X2.append(xi)\n                    y.append(yi)\n                if n == batch_size:\n                    yield ([np.array(X1), np.array(X2)], np.array(y))\n                    X1, X2, y = [], [], []\n                    n = 0","metadata":{"execution":{"iopub.status.busy":"2022-09-05T19:16:27.215288Z","iopub.execute_input":"2022-09-05T19:16:27.215700Z","iopub.status.idle":"2022-09-05T19:16:27.229917Z","shell.execute_reply.started":"2022-09-05T19:16:27.215665Z","shell.execute_reply":"2022-09-05T19:16:27.228884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word Embeddings","metadata":{}},{"cell_type":"code","source":"# transfer learning on text\nf = open(\"../input/glove6b50dtxt/glove.6B.50d.txt\")","metadata":{"execution":{"iopub.status.busy":"2022-09-05T17:59:03.684614Z","iopub.execute_input":"2022-09-05T17:59:03.685180Z","iopub.status.idle":"2022-09-05T17:59:03.691908Z","shell.execute_reply.started":"2022-09-05T17:59:03.685143Z","shell.execute_reply":"2022-09-05T17:59:03.690530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_index = {}\nfor line in f:\n    values = line.split()\n    word = values[0]\n    word_embedding = np.array(values[1:],dtype='float')\n    embedding_index[word] = word_embedding\nf.close()","metadata":{"execution":{"iopub.status.busy":"2022-09-05T17:59:04.852808Z","iopub.execute_input":"2022-09-05T17:59:04.853505Z","iopub.status.idle":"2022-09-05T17:59:08.890908Z","shell.execute_reply.started":"2022-09-05T17:59:04.853468Z","shell.execute_reply":"2022-09-05T17:59:08.889850Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_index['boy']","metadata":{"execution":{"iopub.status.busy":"2022-09-05T17:59:15.419372Z","iopub.execute_input":"2022-09-05T17:59:15.420493Z","iopub.status.idle":"2022-09-05T17:59:15.428796Z","shell.execute_reply.started":"2022-09-05T17:59:15.420436Z","shell.execute_reply":"2022-09-05T17:59:15.427735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_embedding_matrix():\n    emb_dim = 50\n    matrix = np.zeros((vocab_size, emb_dim))\n    for word, idx in word_to_index.items():\n        embedding_vector = embedding_index.get(word)\n        \n        if embedding_vector is not None:\n            matrix[idx] = embedding_vector\n    return matrix","metadata":{"execution":{"iopub.status.busy":"2022-09-05T18:03:47.442444Z","iopub.execute_input":"2022-09-05T18:03:47.443140Z","iopub.status.idle":"2022-09-05T18:03:47.449179Z","shell.execute_reply.started":"2022-09-05T18:03:47.443102Z","shell.execute_reply":"2022-09-05T18:03:47.448223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix = get_embedding_matrix()\nembedding_matrix.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-05T18:04:07.317584Z","iopub.execute_input":"2022-09-05T18:04:07.318034Z","iopub.status.idle":"2022-09-05T18:04:07.334694Z","shell.execute_reply.started":"2022-09-05T18:04:07.317995Z","shell.execute_reply":"2022-09-05T18:04:07.333831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Architecture\n\nimage features and partial sequence ----> |model| ---->  next word in seq\n","metadata":{}},{"cell_type":"code","source":"# output of resnet50 is feed in this\ninput_img_features = Input(shape=(2048,))\ninput_img1 = Dropout(0.3)(input_img_features)\ninput_img = Dense(256, activation='relu')(input_img1)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-05T18:29:48.297975Z","iopub.execute_input":"2022-09-05T18:29:48.298810Z","iopub.status.idle":"2022-09-05T18:29:48.335945Z","shell.execute_reply.started":"2022-09-05T18:29:48.298765Z","shell.execute_reply":"2022-09-05T18:29:48.334460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_captions = Input(shape=(max_len, ))\ninput_cap1 = Embedding(input_dim = vocab_size, output_dim = 50, mask_zero=True)(input_captions)\ninput_cap2 = Dropout(0.3)(input_cap1)\ninput_cap = LSTM(256)(input_cap2) # output size 256","metadata":{"execution":{"iopub.status.busy":"2022-09-05T18:33:02.132779Z","iopub.execute_input":"2022-09-05T18:33:02.133978Z","iopub.status.idle":"2022-09-05T18:33:02.901715Z","shell.execute_reply.started":"2022-09-05T18:33:02.133931Z","shell.execute_reply":"2022-09-05T18:33:02.900657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decoder1 = add([input_img, input_cap])\ndecoder2 = Dense(256, activation='relu')(decoder1)\noutputs = Dense(vocab_size, activation='softmax')(decoder2)","metadata":{"execution":{"iopub.status.busy":"2022-09-05T18:35:55.164989Z","iopub.execute_input":"2022-09-05T18:35:55.165363Z","iopub.status.idle":"2022-09-05T18:35:55.202939Z","shell.execute_reply.started":"2022-09-05T18:35:55.165330Z","shell.execute_reply":"2022-09-05T18:35:55.202038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(inputs=[input_img_features, input_captions], outputs=outputs)","metadata":{"execution":{"iopub.status.busy":"2022-09-05T19:54:04.157840Z","iopub.execute_input":"2022-09-05T19:54:04.158213Z","iopub.status.idle":"2022-09-05T19:54:04.167706Z","shell.execute_reply.started":"2022-09-05T19:54:04.158180Z","shell.execute_reply":"2022-09-05T19:54:04.166590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-09-05T19:54:05.945294Z","iopub.execute_input":"2022-09-05T19:54:05.945688Z","iopub.status.idle":"2022-09-05T19:54:05.952819Z","shell.execute_reply.started":"2022-09-05T19:54:05.945655Z","shell.execute_reply":"2022-09-05T19:54:05.951603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.layers[2].set_weights([embedding_matrix])\nmodel.layers[2].trainable = False # pre trained using transfer learning","metadata":{"execution":{"iopub.status.busy":"2022-09-05T19:19:12.476418Z","iopub.execute_input":"2022-09-05T19:19:12.477117Z","iopub.status.idle":"2022-09-05T19:19:12.484011Z","shell.execute_reply.started":"2022-09-05T19:19:12.477080Z","shell.execute_reply":"2022-09-05T19:19:12.482835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer='adam')","metadata":{"execution":{"iopub.status.busy":"2022-09-05T19:54:09.047791Z","iopub.execute_input":"2022-09-05T19:54:09.048408Z","iopub.status.idle":"2022-09-05T19:54:09.059710Z","shell.execute_reply.started":"2022-09-05T19:54:09.048371Z","shell.execute_reply":"2022-09-05T19:54:09.058366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Model","metadata":{}},{"cell_type":"code","source":"epochs = 20\nbatch_size = 3\nsteps = len(train_descriptions)//batch_size\n\nfor i in range(epochs):\n    generator = data_generator(train_descriptions, encoding_train, word_to_index, max_len, batch_size)\n    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n    model.save('./model_weights/model_'+str(i)+'.h5')","metadata":{"execution":{"iopub.status.busy":"2022-09-05T19:54:10.577939Z","iopub.execute_input":"2022-09-05T19:54:10.578311Z","iopub.status.idle":"2022-09-05T20:05:42.449750Z","shell.execute_reply.started":"2022-09-05T19:54:10.578280Z","shell.execute_reply":"2022-09-05T20:05:42.448577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predictions","metadata":{}},{"cell_type":"code","source":"model = load_model('./model_weights/model_6.h5')","metadata":{"execution":{"iopub.status.busy":"2022-09-05T20:16:40.426554Z","iopub.execute_input":"2022-09-05T20:16:40.427457Z","iopub.status.idle":"2022-09-05T20:16:41.259829Z","shell.execute_reply.started":"2022-09-05T20:16:40.427407Z","shell.execute_reply":"2022-09-05T20:16:41.258605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_caption(photo):\n    text = \"startseq\"\n    for i in range(max_len):\n        sequence = [word_to_index[w] for w in text.split() if w in word_to_index]\n        sequence = pad_sequences([sequence], maxlen=max_len, padding='post')\n        yPred = model.predict([photo, sequence])\n        yPred = yPred.argmax()\n        word = index_to_word[yPred]\n        text += (' '+ word)\n        if word == 'endseq':\n            break\n    \n    final_caption = text.split()[1:-1]\n    final_caption = ' '.join(final_caption)\n    return final_caption","metadata":{"execution":{"iopub.status.busy":"2022-09-05T20:16:42.440300Z","iopub.execute_input":"2022-09-05T20:16:42.440721Z","iopub.status.idle":"2022-09-05T20:16:42.451377Z","shell.execute_reply.started":"2022-09-05T20:16:42.440687Z","shell.execute_reply":"2022-09-05T20:16:42.446755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(15):\n    idx = np.random.randint(0,1000)\n    all_img = list(encoding_test.keys())\n    img_name = all_img[idx]\n    photo = encoding_test[img_name].reshape((1,2048))\n    lol = plt.imread(IMG_PATH+img_name+'.jpg')\n    caption = predict_caption(photo)\n    print(caption)\n    print(descriptions[img_name][0])\n    plt.imshow(lol)\n    plt.axis('off')\n    plt.show()\n    ","metadata":{"execution":{"iopub.status.busy":"2022-09-05T20:16:43.470711Z","iopub.execute_input":"2022-09-05T20:16:43.471448Z","iopub.status.idle":"2022-09-05T20:16:53.647223Z","shell.execute_reply.started":"2022-09-05T20:16:43.471410Z","shell.execute_reply":"2022-09-05T20:16:53.646243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}